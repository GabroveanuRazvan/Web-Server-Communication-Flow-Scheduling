import os
import numpy as np
import pickle as pkl
import json

# Used to read the locust events json generated by other non python programs
COMPUTE_STATS_PATH = "../sctp_benchmarking_script/events_list_10000.json"

# Used in one file benchmarking
FILE_TO_FETCH_PATH = "/3.00M-6.00M/27.html"

# Used to fetch a random file from this root
ROOT = "./benchmark_raw_dataset"


def walk_dir(root: str):
    file_paths = [os.path.join(root, file) for file in os.listdir(root)]

    new_files = []
    for file in file_paths:
        if os.path.isdir(file):
            new_files += walk_dir(file)
        else:
            new_files.append(file)

    return new_files

# Generate or load from a pickle file the list of requests
NUM_REQUESTS = 10000
REQUESTS_LIST_PATH = f"./requests_list_{NUM_REQUESTS}.pkl"

DIRS = [path.removeprefix(ROOT) for path in walk_dir(ROOT)]

def choose_file():
    num_dirs = len(DIRS)
    if num_dirs == 0:
        raise Exception("No benchmark dataset")

    index = np.random.randint(num_dirs)
    return DIRS[index]

def get_requests() -> list[str]:

    if os.path.exists(REQUESTS_LIST_PATH):
        return pkl.load(open(REQUESTS_LIST_PATH, "rb"))

    requests = [choose_file() for _ in range(NUM_REQUESTS)]
    pkl.dump(requests, open(REQUESTS_LIST_PATH, "wb"))
    return requests


def generate_request_dataset(store_dir_path: str,file_name: str,request_count: int,file_count: int):

    os.makedirs(store_dir_path, exist_ok=True)

    for i in range(file_count):
        file_path = os.path.join(store_dir_path, f"{file_name}_{i}.json")
        requests = [choose_file() for _ in range(request_count)]
        with open(file_path, 'w') as f:
            json.dump(requests, f)


if __name__ == "__main__":
    generate_request_dataset("../../requests","requests",10000,12)


